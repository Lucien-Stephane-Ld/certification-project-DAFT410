{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import os\n",
    "\n",
    "from web_scrapping_functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.drinklab.org/cocktail-recipes/'\n",
    "html = requests.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, \"html.parser\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract urls to 'Cocktails by Letter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocktails_by_letter = soup.find_all('div', class_=\"x-text e89516-4 m1x2k-3\")\n",
    "\n",
    "url_by_letter = []\n",
    "\n",
    "for link in cocktails_by_letter:\n",
    "    anchors = link.find_all('a')\n",
    "    for anchor in anchors:\n",
    "        url = anchor['href']\n",
    "        url_by_letter.append(url)\n",
    "\n",
    "\n",
    "url_by_letter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate in the url_by_letter list and get html soup from each from which to extract the url to each individual cocktail page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocktail_urls = []\n",
    "\n",
    "for url in url_by_letter:\n",
    "    base_url = url\n",
    "    start_page = 1\n",
    "\n",
    "    while True:\n",
    "        url = base_url + \"?_page=\" + str(start_page)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        divs = soup.find_all('div', class_=\"col-md-3 col-sm-4 col-xs-6 pt-cv-content-item pt-cv-1-col\")\n",
    "        \n",
    "        \n",
    "        temp_urls = []\n",
    "\n",
    "        for div in divs:\n",
    "            anchor = div.find('a')\n",
    "            if anchor is not None and 'href' in anchor.attrs:\n",
    "                temp_urls.append(anchor[\"href\"])\n",
    "\n",
    "        cocktail_urls.extend(temp_urls)\n",
    "\n",
    "        pagination = soup.find('ul', class_=\"pt-cv-pagination\")\n",
    "        if pagination is None or 'data-totalpages' not in pagination.attrs:\n",
    "            # print('Next subpage')\n",
    "            break\n",
    "        \n",
    "        total_pages = int(pagination['data-totalpages'])\n",
    "        if start_page >= total_pages:\n",
    "            # print('Next page')\n",
    "            break\n",
    "        \n",
    "        start_page += 1\n",
    "\n",
    "    # print(cocktail_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cocktail_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocktail_urls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract relevant info for each cocktail from cocktail_urls list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists to receive scrapped material, to be concatenated into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctl_names = []\n",
    "ctl_description = [] \n",
    "ctl_preptime = []\n",
    "ctl_ingredients = []\n",
    "ctl_recipe = []\n",
    "ctl_vid = []\n",
    "ctl_nutrition = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "une mega loop qui récup_re toute les soups, les write en html.\n",
    "et ensuite on va chercher dans le soups tous les trucs que je veux (name, description, etc...)\n",
    "+\n",
    "add timer around get"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap and stock locally all HTML soups, for futur iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = ['https://www.drinklab.org/american-beauty-cocktail/',\n",
    " 'https://www.drinklab.org/aviation-gin-riblet/',\n",
    " 'https://www.drinklab.org/mason-jar-aperol-spritz/',\n",
    " 'https://www.drinklab.org/americano/',\n",
    " 'https://www.drinklab.org/amaretto-sour-2/',\n",
    " 'https://www.drinklab.org/aperol-spritz/',\n",
    " 'https://www.drinklab.org/airedale-cocktail/',\n",
    " 'https://www.drinklab.org/articuno-pokemon-cocktail/',\n",
    " 'https://www.drinklab.org/alfonso-2/',\n",
    " 'https://www.drinklab.org/azzuro-2/',\n",
    " 'https://www.drinklab.org/american-leroy-2/',\n",
    " 'https://www.drinklab.org/angels-tit/',\n",
    " 'https://www.drinklab.org/alex-chi-chi-2/',\n",
    " 'https://www.drinklab.org/aqua-2/',\n",
    " 'https://www.drinklab.org/apple-colada-2/',\n",
    " 'https://www.drinklab.org/arago-2/',\n",
    " 'https://www.drinklab.org/arizona-twister-2/',\n",
    " 'https://www.drinklab.org/andreas-colada-collision-2/',\n",
    " 'https://www.drinklab.org/almond-cooler-2/',\n",
    " 'https://www.drinklab.org/almond-joy-2-3/',\n",
    " 'https://www.drinklab.org/angels-tip-2-2/',\n",
    " 'https://www.drinklab.org/american-glory-2/',\n",
    " 'https://www.drinklab.org/american-rose-2/',\n",
    " 'https://www.drinklab.org/american-flyer-2/',\n",
    " 'https://www.drinklab.org/aqua-marina-2/' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soups(url_list):\n",
    "\n",
    "    cocktail_htmls = []\n",
    "    file_counter = 622\n",
    "    file_path = 'C:/Users/User/Desktop/FORMATION/IRONHACK/PROJECTS/PROJECT-FINAL/html_soup'\n",
    "\n",
    "    for url in tqdm_notebook(url_list):\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "        cocktail_htmls.append(soup)\n",
    "\n",
    "        filename = f'cocktail_{file_counter}.html'\n",
    "\n",
    "        file_full_path = os.path.join(file_path, filename)\n",
    "        \n",
    "        with open(file_full_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(str(soup))\n",
    "\n",
    "        file_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14140\\3470693433.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for url in tqdm_notebook(url_list):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926e670506154c6aaacf9778e08248d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_soups(cocktail_urls[622:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/User/Desktop/FORMATION/IRONHACK/PROJECTS/PROJECT-FINAL/html_soup'\n",
    "\n",
    "# extracted_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_info(x):\n",
    "\n",
    "    extracted_info = []\n",
    "\n",
    "    for file_name in os.listdir(x):\n",
    "        if file_name.endswith('.html'):\n",
    "            file_full_path = os.path.join(x, file_name)\n",
    "            \n",
    "            with open(file_full_path, 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file, 'html.parser')\n",
    "                \n",
    "                # name scrap\n",
    "                name = soup.find('h1', class_=\"wprm-recipe-name wprm-block-text-bold\")\n",
    "                if name is not None:\n",
    "                    ctl_name = name.text\n",
    "                else:\n",
    "                    ctl_name = None\n",
    "                if ctl_name is None:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # description scrap\n",
    "                description = soup.find('div', class_=\"wprm-recipe-summary wprm-block-text-normal\")\n",
    "\n",
    "                if description is not None:\n",
    "                    description_txt = soup.find('div', class_=\"wprm-recipe-summary wprm-block-text-normal\").text\n",
    "                    ctl_description.append(description_txt)\n",
    "\n",
    "                else:\n",
    "                    ctl_description.append(\"Unfortunately, we have no description for this drink... You'll have to describe it yourself!\")\n",
    "                \n",
    "\n",
    "                # recipe scrap\n",
    "                recipe = soup.find('div', class_=\"wprm-recipe-instruction-group\")\n",
    "                if recipe is not None:\n",
    "                    ctl_recipe = recipe.text\n",
    "                else:\n",
    "                    ctl_recipe = \"Woops... We couldn't retrieve the exact recipe... It's trial & error time! Just a little more fun before enjoying a nice drink!\"\n",
    "                \n",
    "\n",
    "                # ingredients scrap\n",
    "                ingredients = soup.find('div', class_=\"wprm-recipe-ingredient-group\")\n",
    "                if ingredients is not None:\n",
    "                    ctl_ingredients = ingredients.text.replace('▢', ' |').replace('  ', ':')\n",
    "                else:\n",
    "                    ctl_ingredients = \"Woops... What happened ?! Something didn't work, please try again.\"    \n",
    "                # ingredients = [tag.text.strip() for tag in ingredient_tags]\n",
    "\n",
    "\n",
    "                # video link scrap\n",
    "                video = soup.find('iframe')\n",
    "                if video is not None:\n",
    "                    ctl_vid = video['src']\n",
    "                else:\n",
    "                    ctl_vid = \"There doesn't seem to be an instructional video for this cocktail. Why not make the tutorial yourself!\"\n",
    "\n",
    "                \n",
    "                # append all scrapped info to list\n",
    "                extracted_info.append([ctl_name, ctl_description, ctl_recipe, ctl_ingredients, ctl_vid])\n",
    "\n",
    "    # Create dataframe from extracted_info list.\n",
    "    df = pd.DataFrame(extracted_info, columns=['Name', 'Description', 'Recipe', 'Ingredients', 'Video Link'])\n",
    "\n",
    "    # Print dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctl_df = scrap_info(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctl_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap cocktail names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(url_list) :\n",
    "      \n",
    "      for url in tqdm_notebook(url_list):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        names = soup.find('h1', class_=\"wprm-recipe-name wprm-block-text-bold\")\n",
    "      if names is None :\n",
    "         print('Could not scrap ctl_name for {url}')\n",
    "      else:\n",
    "         ctl_names.append(names.text)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_names(cocktail_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(url_list):\n",
    "\n",
    "   for url in url_list:\n",
    "      response = requests.get(url)\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "      description = soup.find('div', class_=\"wprm-recipe-summary wprm-block-text-normal\")\n",
    "\n",
    "      if description is not None:\n",
    "         description_txt = soup.find('div', class_=\"wprm-recipe-summary wprm-block-text-normal\").text\n",
    "         ctl_description.append(description_txt)\n",
    "\n",
    "      else:\n",
    "         ctl_description.append(\"Unfortunately, we have no description for this drink... You'll have to describe it yourself!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_description(cocktail_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap prep time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prep_time(url_list):\n",
    "\n",
    "   for url in url_list:\n",
    "      response = requests.get(url)\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "      prep_time = soup.find('div', class_=\"wprm-recipe-block-container wprm-recipe-block-container-columns wprm-block-text-normal wprm-recipe-time-container wprm-recipe-total-time-container\")\n",
    "\n",
    "      if prep_time is not None:\n",
    "         prep_time_txt = prep_time.text.replace(' minutes minutes',' minutes')\n",
    "         ctl_preptime.append(prep_time_txt)\n",
    "\n",
    "      else:\n",
    "         ctl_preptime.append(\"No time estimate for the preparation... Only one way to find out!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prep_time(cocktail_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingredients(url_list):\n",
    "\n",
    "   for url in url_list:\n",
    "      response = requests.get(url)\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "      # ingredients = soup.find('div' , class_=\"wprm-recipe-ingredients-container wprm-recipe-ingredients-no-images wprm-recipe-87010-ingredients-container wprm-block-text-normal wprm-ingredient-style-regular wprm-recipe-images-before\")\n",
    "      ingredients = soup.find('div', class_=\"wprm-recipe-ingredient-group\")\n",
    "\n",
    "      if ingredients is not None:\n",
    "         ingredients_txt = ingredients.text.replace('▢',' |').replace('  ',':')\n",
    "\n",
    "         ctl_ingredients.append(ingredients_txt)\n",
    "\n",
    "      else:\n",
    "         ctl_ingredients.append(\"Woops... What happened ?! Something didn't work, please try again.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ingredients(cocktail_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipe(url_list):\n",
    "\n",
    "   for url in url_list:\n",
    "      response = requests.get(url)\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "      recipe = soup.find('div', class_=\"wprm-recipe-instruction-group\")\n",
    "\n",
    "      if recipe is not None:\n",
    "         recipe_txt = recipe.text\n",
    "\n",
    "         ctl_recipe.append(recipe_txt)\n",
    "\n",
    "      else:\n",
    "         ctl_recipe.append(\"Woops... We couldn't retrieve the exact recipe... It's trial & error time! Just a little more fun before enjoying a nice drink!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recipe(cocktail_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video(url_list):\n",
    "\n",
    "   for url in url_list:\n",
    "      response = requests.get(url)\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "      video = soup.find('iframe')\n",
    "\n",
    "      if video is not None:\n",
    "         video_url = video['src']\n",
    "         ctl_vid.append(video_url)\n",
    "\n",
    "      else:\n",
    "         ctl_vid.append(\"There doesn't seem to be an instructional video for this cocktail. Why not make the tutorial yourself!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_video(cocktail_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap nutrition facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nutrition(url_list):\n",
    "\n",
    "   for url in url_list:\n",
    "      response = requests.get(url)\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "      nutrition = soup.find('div', class_=\"wprm-nutrition-label-container wprm-nutrition-label-container-simple wprm-block-text-normal\")\n",
    "\n",
    "      if nutrition is not None:\n",
    "         nutrition_txt = nutrition.text\n",
    "         ctl_nutrition.append(nutrition_txt)\n",
    "\n",
    "      else:\n",
    "         ctl_nutrition.append(\"There doesn't seem to be any nutritional facts associated to this cocktail. But we know that's not why you're here...\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nutrition(cocktail_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if scrap successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ctl_names) == len(cocktail_urls):\n",
    "     print('Ctl Names = OK!')\n",
    "     else print('Ctl Names = TO CHECK!')\n",
    "\n",
    "if len(ctl_description) == len(cocktail_urls):\n",
    "     print('Ctl Names = OK!')\n",
    "     else print('Ctl Names = TO CHECK!')\n",
    "\n",
    "if len(ctl_preptime) == len(cocktail_urls):\n",
    "     print('Ctl Names = OK!')\n",
    "     else print('Ctl Names = TO CHECK!')\n",
    "\n",
    "if len(ctl_ingredients) == len(cocktail_urls):\n",
    "     print('Ctl Names = OK!')\n",
    "     else print('Ctl Names = TO CHECK!')\n",
    "\n",
    "if len(ctl_recipe) == len(cocktail_urls):\n",
    "     print('Ctl Names = OK!')\n",
    "     else print('Ctl Names = TO CHECK!')\n",
    "\n",
    "if len(ctl_vid) == len(cocktail_urls):\n",
    "     print('Ctl Names = OK!')\n",
    "     else print('Ctl Names = TO CHECK!')\n",
    "\n",
    "if len(ctl_nutrition) == len(cocktail_urls):\n",
    "     print('Ctl Names = OK!')\n",
    "     else print('Ctl Names = TO CHECK!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________\n",
    "\n",
    "### Debug the webscrapping function\n",
    "\n",
    "return the names in the functions and deal with the append here.  \n",
    "OR  \n",
    "paass the list in the function and return the filled list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def webscrapping(x):\n",
    "#     get_names(x)\n",
    "#     get_description(x)\n",
    "#     get_prep_time(x)\n",
    "#     get_ingredients(x)\n",
    "#     get_recipe(x)\n",
    "#     get_video(x)\n",
    "#     get_nutrition(x)\n",
    "\n",
    "#     ctl_df = pd.DataFrame(list(zip(ctl_names,ctl_description,ctl_ingredients,ctl_recipe,ctl_preptime,ctl_vid,ctl_nutrition)))\n",
    "#     ctl_df = ctl_df.set_axis([\"Cocktail\",\"Description\",\"Ingredients\",\"Recipe\",\"Prep. Time\",\"Video Link\",\"Nutritional Facts\"], axis=\"columns\")\n",
    "#     return ctl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webscrapping(test_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctl_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
